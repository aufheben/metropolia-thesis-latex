% Project Specifications

\chapter{The Generator Model}

In this chapter, a brief description of \gls{gan} is presented first, followed by the structure of the
generator in the \gls{dcgan} model used in this project. There are three types of layers in the model:
a transposed convolutional layer that performs upscaling to its input, a batch normalization layer that
improves model stability and accuracy, an activation layer that introduces nonlinearity to the model.
The main focus is on the transposed convolutional layer, which is the core of the generator model.

\section{A Description of \gls{gan}}

There are two networks in a \gls{gan} model: $D$, the discriminator, and $G$, the generator. $D$ is a
discriminative model which computes a function $D: \boldsymbol{x} \rightarrow p$ where $\boldsymbol{x}$ is the
input example and $p \in \mathbb{R}$ is the probability that $\boldsymbol{x}$ came from real training data
rather than data generated by $G$. In a sense, this probability value identifies the input example as
``authentic'' or not, so the higher the probability, the better $D$ does at discriminating authentic data
against data ``faked'' by $G$. $G$, on the other hand, tries to fool $D$ by generating output that resembles
the real data, and learns the data distribution during the training process. The input to $G$ is a vector
of random noises $\boldsymbol{z}$ which could be drawn from a normal distribution, so $G: \boldsymbol{z}
\rightarrow \boldsymbol{x}$ is a mapping from the $\boldsymbol{z}$ space to the data space $\boldsymbol{x}$.

During the training, two types of examples are fed to $D$: existing training examples and examples generated
by $G$. The system can be trained with regular stochastic gradient descent and backpropagation. The training
process improves the ability of both $D$ and $G$, until eventually the output of $G$ will be
indistinguishable from real world examples to $D$. Once trained, $D$ can be discarded and $G$ can be used
in different applications.

In the original paper both networks are \glspl{mlp}. However, many different network types have been
proposed since then. In this project, $D$ and $G$ are both deep convolutional neural networks which are
suitable for image processing. The training of $D$ and $G$ is done on GPU with floating-point numbers. Since
$D$ is discarded after training, from now on we are only concerned with $G$.

\section{DCGAN Network Structure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{network_structure}
  \caption{\gls{dcgan} Network Structure}
  \label{fig:network_structure}
\end{figure}

Figure \ref{fig:network_structure} shows the network structure of $G$, with five transposed convolutional
layers. Except for the last transposed convolutional layer, each of the previous four transposed convolutional
layers is followed by a layer of batch normalization, and a layer of \gls{relu} for
activation. Activation functions introduces nonlinearity to the network. The last transposed convolutional
layer is followed by a layer of $tanh$ function applied to each element as activation. The parameters for
each layer is detailed in Table \ref{table:network_layers}.  The network structure is rather simple,
compared with many other much larger networks. ResNet, for example, contains a deep cascade of 152 layers.

\begin{table}[h]
  \centering
  \caption{\gls{dcgan} Layers: TC stands for transposed convolution, BN stands for batch normalization}
  \begin{tabular}{l | l | l }
    \toprule
    Layer & Type & Description \\
    \midrule
    1 & TC & $100 \rightarrow 512$ channels, $4 \times 4$ kernel, stride $1$, padding $0$ \\
    2 & BN & $512$ channels\\
    3 & ReLU & $512$ channels \\
    4 & TC & $512 \rightarrow 256$ channels, $4 \times 4$ kernel, stride $2$, padding $1$ \\
    5 & BN & $256$ channels \\
    6 & ReLU & $256$ channels \\
    7 & TC & $256 \rightarrow 128$ channels, $4 \times 4$ kernel, stride $2$, padding $1$ \\
    8 & BN & $128$ channels \\
    9 & ReLU & $128$ channels \\
    10 & TC & $128 \rightarrow 64$ channels, $4 \times 4$ kernel, stride $2$, padding $1$ \\
    11 & BN & $64$ channels \\
    12 & ReLU & $64$ channels \\
    13 & TC & $64 \rightarrow 3$ channels, $4 \times 4$ kernel, stride $2$, padding $1$ \\
    14 & Tanh & 3 channels, final layer \\
    \bottomrule
  \end{tabular}
  \label{table:network_layers}
\end{table}

\section{Transposed Convolutional Layer}

In \gls{cnn}s, convolutional layer extracts various features from the input, essentially performing a
downsampling operation. Transposed convolutional layer, also known as fractionally strided convolutional
layers, or sometime erroneously as deconvolutional layer, on the other hand performs upsampling on the input.
Normally unsampling is often done with interpolation, but transposed convolution as a novel approach offers
trainability which makes it useful for neural networks. Conceptually, if a convolution layer of stride $f$
is run backwards, it can be seen as convolution layer with stride $1/f$, hence the name fractionally
strided convolution.

\subsection{Convolution}

To understand the operation of transposed convolution, it is perhaps natural to start with regular convolution
first. It is also helpful to work though a simple numerical example, starting with a $2D$ convolution
with an input of $4 \times 4$ matrix $A$:

$$
A =
  \begin{pmatrix}
    1 & 2 & 3 & 4 \\
    4 & 3 & 2 & 1 \\
    1 & 2 & 3 & 4 \\
    4 & 3 & 2 & 1
  \end{pmatrix}
$$

Let the kernel be a $2 \times 2$ matrix $K$:

$$
K =
  \begin{pmatrix}
    1 & 2 \\
    3 & 4
  \end{pmatrix}
$$

Assume the convolution operates with padding of $1$ and stride of $2$, that is, $K$ is slid across the
zero-padded matrix $B$ with a step of $2$, from left to right, top to bottom. $B$ is shown below:

$$
B =
  \begin{pmatrix}
    {\color{red}0} & {\color{red}0} & {\color{blue}0} & {\color{blue}0} & 0 & 0 \\
    {\color{red}0} & {\color{red}1} & {\color{blue}2} & {\color{blue}3} & 4 & 0 \\
    0 & 4 & 3 & 2 & 1 & 0 \\
    0 & 1 & 2 & 3 & 4 & 0 \\
    0 & 4 & 3 & 2 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0
  \end{pmatrix}
$$

When $K$ is slid across $B$, the overlapping entries in $B$ is called a \textit{patch}. The first two
patches are shown in red and blue respectively. At each step, an element-wise inner product (Frobenius inner
product) $\langle K,P \rangle_{F}$ is computed between $K$ and the corresponding patch $P$ and stored as
an element in the result matrix $C$:

$$
C =
  \begin{pmatrix}
    4 & 8 & 12 \\
    12 & 25 & 13 \\
    8 & 7 & 1
  \end{pmatrix}
$$

In practice, the sliding-window approach is inefficient for implementation. All practical implementations
use a pair of operations called $im2col$ and $col2im$ to wrap a single matrix multiplication. Since
matrix multiplication is such a fundamental operation that its algorithm has been highly optimized over the
decades, and this vindicates the memory overhead caused by $im2col$. To start, flatten $K$ into a row vector
$K_{row}$:

$$
K_{row} =
  \begin{pmatrix}
    1 & 2 & 3 & 4 \\
  \end{pmatrix}
$$

For each patch in $B$ that $K$ convolves with, the entries in that patch are unrolled into a matrix $B_{col}$
made of column vectors:

$$
B_{col} =
  \begin{pmatrix}
    0 & 0 & 0 & 0 & 3 & 1 & 0 & 3 & 1 \\
    0 & 0 & 0 & 4 & 2 & 0 & 4 & 2 & 0 \\
    0 & 2 & 4 & 0 & 2 & 4 & 0 & 0 & 0 \\
    1 & 3 & 0 & 1 & 3 & 0 & 0 & 0 & 0
  \end{pmatrix}
$$

This operation is called $im2col$, namely, image to columns. Now, compute the product $K_{row} * B_{col}$, we
obtain a $1 \times 9$ matrix:

$$
\begin{pmatrix}
  4 & 18 & 12 & 12 & 25 & 13 & 8 & 7 & 1
\end{pmatrix}
$$

Finally this matrix is "reshaped" to the desired $3 \times 3$ output which is $C$ using the operation $col2im$.

\subsection{Represent Convolution as a Sparse Matrix}

The procedure described in the previous section is how regular convolution would be implemented in practice.
On the other hand, there is an alternative view of the convolution, also performed with a single
matrix multiplication. This alternative view is impractical for implementation, but from which
input and output can be easily reversed. To see this, first unroll the zero-padded input $B$ into
a $36 \times 1$ matrix:

\setcounter{MaxMatrixCols}{20}

$$
B'^\intercal =
  \begin{pmatrix}
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 2 & 3 & 4 & 0 & 0 & 4 & \dots
  \end{pmatrix}
$$

The output $C$ is also unrolled into a $9 \times 1$ matrix:

$$
C'^\intercal =
  \begin{pmatrix}
    4 & 18 & 12 & 12 & 25 & 13 & 8 & 7 & 1
  \end{pmatrix}
$$

Then the convolution can be represented as a sparse matrix $M$ of $9 \times 36$ with entries from kernel $K$,
one patch per row:

$$
M =
  \begin{pmatrix}
    1 & 2 & 0 & 0 & 0 & 0 & 3 & 4 & 0 & 0 & 0 & 0 & \dots \\
    0 & 0 & 1 & 2 & 0 & 0 & 0 & 0 & 3 & 4 & 0 & 0 & \dots \\
    \vdots \\
  \end{pmatrix}
$$

$M$ takes $B'$ as input and produces $C'$ as output, which again can be reshaped to $C$.

\subsection{Transposed Convolution}

In the previous section, the convolution is represented as a kernel-defined sparse matrix. Under this view,
it is very easy to derive the \textit{backward pass}. This example convolution maps an input of $4 \times 4$
to an output of $3 \times 3$, to reverse the direction of input and output, namely, to take an input of
$3 \times 3$ and produce an output of $4 \times 4$, all it takes is to transpose $M$ to obtain $M^\intercal$
of $36 \times 9$. When $M^\intercal$ is applied to the an $3 \times 3$ input unrolled to $9 \times 1$, the
resulting $36 \times 1$ matrix is then reshaped to the desired $4 \times 4$ output. Hence the name
\textit{transposed convolution}.

It needs to be pointed out that the term \textit{backward pass} instead of \textit{inverse operation},
since $M^\intercal$ does not recover the numerical values of $B$ from $C$, it only recovers the shape of $B$,
therefore it is misleading to call this operation deconvolution.

\section{Batch Normalization Layer}

Batch normalization is an operation defined as

\begin{equation} \label{eq:batch_normalization}
  y = \gamma \frac{x - \bar{x}}{\sqrt{\sigma^2(x) + {eps}}} + \beta
\end{equation}

Here $eps$ is a small constant that adds to numerical stability. $\gamma$ and $\beta$ can be regarded
as trained weight and bias. The output $y$ of the normalization would have a mean of zero and standard
deviation of one. Batch normalization optimizes the training of the network, making it converge
faster with higher learning rates. It can also improve overall accuracy of the model. For deep networks
like \gls{dcgan}, it is a key ingredient.

However, this operation involves an inversed square root operation $\frac{1}{\sqrt{\sigma^2(x) + eps}}$,
which is rather difficult to compute with fixed-point numbers. Luckily Xilinx Vivado includes an IP which
can both convert between fixed-point and floating-point representations, as well as compute inversed square
root.

\section{Activation Layer}

Activation function, also known as \textit{transfer function}, adds non-linearity to the network. It is often
attached to the output of a layer, typically mapping the results to the range $(0, 1)$ or $(-1, 1)$, although
other possibilities exist. Assume the functions maps the input to range $(0, 1)$, a value close to $0$ would
be seen as "off" or "no", a value close to $1$ would be seen as "on" or "yes". This indicates whether the
following connection should see this output as activated or not, hence the name activation function.

Many different types of activation functions exist with two main categories: linear and nonlinear. A commonly
used sigmoid (S-shaped) function is $y = \frac{1}{1 + e^{-x}}$, shown in \ref{fig:sigmoid_relu}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{sigmoid_relu}
  \caption{Sigmoid vs. ReLU}
  \label{fig:sigmoid_relu}
\end{figure}

\subsection{ReLU Layer}

ReLU is very popular nowadays. It is defined as

\begin{equation} \label{eq:relu}
  y = max(0, x)
\end{equation}

The first thing to notice is that it is in fact nonlinear. When compared with the sigmoid function, the
gradient of ReLU does not saturate when $x$ gets large, which makes \gls{sgd} convergence faster. In addition,
since all negative values are converted to zero, it adds the desirable feature of sparsity to the network,
making computation efficient. The implementation of ReLU is of course straightforward.

\subsection{Tanh Layer}

The hyperbolic tangent function $tanh$ is another widely used activation function. It works similarly to the
sigmoid function with the additional property of being symmetrical with respect to the origin.

\begin{equation} \label{eq:tanh}
  \begin{split}
    y & = \frac{e^x - e^{-x}}{e^{x} + e^{-x}} \\
      & = \frac{e^{2x} - 1}{e^{2x} + 1} \\
      & = \frac{1- e^{-2x}}{1 - e^{-2x}}
  \end{split}
\end{equation}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{sigmoid_tanh}
  \caption{Sigmoid vs. Tanh}
  \label{fig:sigmoid_tanh}
\end{figure}

\clearpage %force the next chapter to start on a new page. Keep that as the last line of your chapter!
