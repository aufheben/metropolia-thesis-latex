% Project Specifications

\chapter{GEMM and $col2im$}

The previous chapter discussed different ways to view the convolution operation and its cousin,
the transposed convolution, both conceptually and implementationally. When it comes down to implementation,
we have seen that both operations can be implemented with a single matrix multiplication. In other words,
matrix multiplication is the main computational burden of these operations. Meanwhile, real world applications
often involve very large matrices. Therefore, as a low-level operation, the implementation of matrix
multiplication is often heavily optimized.

\gls{gemm} is considered as the \textit{de facto} standard operation
contained in the \gls{blas} specification. It has many implementations for
different platforms which exhaustively utilize various optimizations like \gls{simd} instructions,
parallelism, cache-awareness, etc.

\gls{gemm} is defined as

$$C \leftarrow \alpha op(A) op(B) + \beta C,$$

where $\alpha, \beta \in \mathbb{R}$, $op(X)$ is either $X$ or $X^\intercal$. In our particular case for
transposed convolution, $\alpha = 1$, $\beta = 0$, $op(X) = X$, so \gls{gemm} reduces to the basic
matrix multiplication $C \leftarrow A B$.

A subtle detail in the implementation of \gls{gemm} is the order of storage of matrix entries. There are two
different orders to store the same matrix: row-major order or column-major order. In row-major order,
entries of rows are stored contiguously in memory, while in column-major order, entries of columns are
consecutive to each other in memory.

A naïve implementation of \gls{gemm} is shown in Listing \ref{code:naivegemm}. Here,
\mintinline{c}{lda}, \mintinline{c}{ldb} and \mintinline{c}{ldc} are
leading dimension of matrix $A$, $B$ and $C$, respectively.

\begin{code}
\begin{minted}{c}
for (int i = 0; i < m; i++) {
    for (int j = 0; j < n; j++) {
        float sum = 0;
        for (int l = 0; l < k; l++)
          sum += a[l*lda+i] * b[l*ldb+j];
        c[j*ldc+i] = beta * c[j*ldc+i] + alpha * sum;
    }
}
\end{minted}
  \captionof{listing}{Naïve C Implementation of \gls{gemm}}
\label{code:naivegemm}
\end{code}

This implementation is straightforward and not very efficient, but it is a good starting point to guide
us through the rest of the design.

Another operation used by transposed convolution is $col2im$. It cherry picks the elements computed by
\gls{gemm} and places them in the destination image. To describe $col2im$, it is necessary to introduce
the variables used for transposed convolution.

\begin{itemize}
  \item \mintinline{c}{inputHeight} and \mintinline{c}{inputWidth} are the height and width of each input
    channel (sometimes also called input plane)
  \item \mintinline{c}{nInputChannel} is the number of input channel in total, thus the input is a
    $3D$ tensor with dimensions \mintinline{c}{(nInputChannel, inputHeight, inputWidth)}
  \item With \mintinline{c}{m = inputHeight * inputWidth} and \mintinline{c}{k = nInputChannel}, the input
    tensor is stored as a matrix $A \in \mathbb{R}^{m \times k}$
  \item \mintinline{c}{kernelH} and \mintinline{c}{kernelW} are the height and width of each
    convolutional kernel channel
  \item Each kernel is also a $3D$ tensor with dimensions \mintinline{c}{(nInputChannel, kernelH, kernelW)}
  \item \mintinline{c}{nOutputChannel} is the number of output channels
  \item There are \mintinline{c}{nOutputChannel}'s of kernels, each responsible for producing an output channel
  \item With \mintinline{c}{k = nInputChannel} and \mintinline{c}{n = nOutputChannel * kernelH * kernelW},
    the kernel tensor is stored as a matrix $B \in \mathbb{R}^{k \times n}$
\end{itemize}

The code of $col2im$ is given in listing \ref{code:col2im}:

\begin{code}
\begin{minted}{c}
// content of data_im set to 0 already
const int n = nOutputChannel * kernelH * kernelW;
for (int j = 0; j < n; ++j) {
    int w_offset = j % kernelW;
    int h_offset = (j / kernelW) % kernelH;
    int c_im = j / kernelH / kernelW;
    for (int h_col = 0; h_col < inputHeight; ++h_col) {
        for (int w_col = 0; w_col < inputWidth; ++w_col) {
            int h_im = h_col * strideH - padH + h_offset * dilationH;
            int w_im = w_col * strideW - padW + w_offset * dilationW;
            if (h_im >= 0 && h_im < outputHeight &&
                w_im >= 0 && w_im < outputWidth) {
                data_im[(c_im * outputHeight + h_im) * outputWidth + w_im] +=
                    data_col[(j * inputHeight + h_col) * inputWidth + w_col];
            }
        }
    }
}
\end{minted}
\captionof{listing}{C Implementation of $col2im$}
\label{code:col2im}
\end{code}

Here \mintinline{c}{data_im} is the target image while \mintinline{c}{data_col} is the result of \gls{gemm}.
The outermost loop iterates through the columns of $C \in \mathbb{R}^{k \times n}$. The two nested loops
iterate the rows of $C$, since \mintinline{c}{m = inputHeight * inputWidth}. \mintinline{c}{c_im} is the
index of the channel in the output image. %\mintinline{c}{h_offset} and \mintinline{c}{w_offset} are the offset in each 

For transposed convolution, \mintinline{c}{data_im} is typically smaller than
\mintinline{c}{data_col}. This leads to a key optimization idea: it is possible to merge \gls{gemm} and
$col2im$ together and compute transposed convolution by doing \gls{gemm} sparsely. In doing so,
\mintinline{c}{data_col} can be abandoned and the results will be directly placed in \mintinline{c}{data_im},
that is, there is no need for an extra large buffer for intermediate results.

\begin{code}
\begin{minted}{c}
int j = 0;
for (int c_im = 0; c_im < nOutputChannel; c_im++) {
    for (int h_offset = 0; h_offset < kernelH; h_offset++) {
        for (int w_offset = 0; w_offset < kernelW; w_offset++) {
            int i = 0;
            for (long h_col = 0; h_col < inputHeight; h_col++) {
                for (long w_col = 0; w_col < inputWidth; w_col++) {
                    int h_im = h_col * strideH - padH + h_offset * dilationH;
                    int w_im = w_col * strideW - padW + w_offset * dilationW;
                    if (h_im >= 0 && h_im < outputHeight &&
                        w_im >= 0 && w_im < outputWidth) {
                        float sum = 0;
                        for (long l = 0; l < k; l++)
                            sum += a[l*lda + i] * b[l*ldb + j];
                        int idx = (c_im * outputHeight + h_im) *
                                  outputWidth + w_im;
                        data_im[idx] += sum;
                    }
                    i++;
                }
            }
            j++;
        }
    }
}
\end{minted}
\captionof{listing}{C Implementation of $transposed\_convolution$}
\label{code:transposed_convolution_in_c}
\end{code}

The key to understand this code of merge is to notice that the for-loop indexed by \mintinline{c}{j} in
\mintinline{c}{gemm} can be splitted into three nested for-loop indexed by \mintinline{c}{c_im},
\mintinline{c}{h_offset} and \mintinline{c}{w_offset}.
Similarly, the for-loop indexed by \mintinline{c}{i} in \mintinline{c}{gemm} is equivalent to the two
nested for-loop indexed by \mintinline{c}{h_col} and \mintinline{c}{w_col}, same as in \mintinline{c}{col2im}.

This last C code in listing \ref{code:transposed_convolution_in_c} serves as the blueprint for implementing
transposed convolution on \gls{fpga} with Verilog.

\clearpage %force the next chapter to start on a new page. Keep that as the last line of your chapter!
