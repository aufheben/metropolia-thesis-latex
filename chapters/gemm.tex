% Project Specifications

\chapter{GEMM and $col2im$}

The previous chapter discussed about different ways to view the convolution operation and its cousin,
the transposed convolution, both conceptually and implementationally. When it comes down to implementation,
we have seen that both operations can be implemented with a single matrix multiplication. In other words,
matrix multiplication is the main computational burden of these operations, meanwhile, real world applications
often involve very large matrices. Therefore, as a low-level operation, the implementation of matrix
multiplication is often heavily optimized.

General matrix multiplication (GEMM) is considered as the \textit{de facto} standard operation
contained in the Basic Linear Algebra Subprograms (BLAS) specification. It has many implementations for
different platforms, and utilizes optimizations like vectorization (e.g. SSE), SIMD-style intructions,
parallelism, cache-awareness, etc.


GEMM is defined as

$$C \leftarrow \alpha op(A) op(B) + \beta C,$$

where $\alpha, \beta \in \mathbb{R}$, $op(X)$ is either $X$ or $X^\intercal$. In our particular case for transposed convolution, $\alpha = 1$, $\beta = 1$, $op(X) = X$, so GEMM reduces to

$$C \leftarrow A B + C$$

where C is the bias matrix to be added to the product $A B$.

A subtle detail in the implementation of GEMM is the order of storage of matrix entries. There are two
different orders to store the same matrix: row-major order or column-major order. In row-major order,
entries of rows are stored contiguously in memory, while in column-major order, entries of columns are
consecutive to each other in memory.

A näive implementation of GEMM is shown in \ref{code:naivegemm}. Here, $lda$, $ldb$ and $ldc$ are leading
dimension of matrix $A$, $B$ and $C$, respectively.

\begin{code}
\begin{minted}{c}
for (int i = 0; i < m; i++) {
    for (int j = 0; j < n; j++) {
        float sum = 0;
        for (int l = 0; l < k; l++)
          sum += a[l*lda+i] * b[l*ldb+j];
        c[j*ldc+i] = beta * c[j*ldc+i] + alpha * sum;
    }
}
\end{minted}
\captionof{listing}{Näive C Implementation of GEMM}
\label{code:naivegemm}
\end{code}

This implementation is straight and not very efficient, but it will serve as the blueprint for implementing
transposed convolution on FPGA.

Another operation used by transposed convolution is $col2im$. It cherry picks the elements computed by GEMM
and places them in the destination image. The code is given below:

\begin{code}
\begin{minted}{c}
// content of data_im set to 0 already
const int n = nOutputChannel * kernelH * kernelW;
for (int j = 0; j < n; ++j) {
    int w_offset = j % kernelW;
    int h_offset = (j / kernelW) % kernelH;
    int c_im = j / kernelH / kernelW;
    for (int h_col = 0; h_col < inputHeight; ++h_col) {
        for (int w_col = 0; w_col < inputWidth; ++w_col) {
            int h_im = h_col * strideH - padH + h_offset * dilationH;
            int w_im = w_col * strideW - padW + w_offset * dilationW;
            if (h_im >= 0 && h_im < outputHeight &&
                w_im >= 0 && w_im < outputWidth) {
                data_im[(c_im * outputHeight + h_im) * outputWidth + w_im] +=
                    data_col[(j * inputHeight + h_col) * inputWidth + w_col];
            }
        }
    }
}
\end{minted}
\captionof{listing}{C Implementation of $col2im$}
\label{code:col2im}
\end{code}

Here $data\_im$ is the target image while $data\_col$ is the result of GEMM. The rest is self-explanatory.
For transposed convolution, $data\_im$ is typically smaller than $data\_col$. This leads a key optimization
idea: it is possible to merge GEMM and $col2im$ together and compute transposed convolution by doing GEMM
sparsely. In doing so, $data\_col$ can be abandomed and the results will be directly placed in $data\_im$,
that is, there is no need for an extra large buffer for intermediate results.

\begin{code}
\begin{minted}{c}
int j = 0;
for (int c_im = 0; c_im < nOutputChannel; c_im++) {
    for (int h_offset = 0; h_offset < kH; h_offset++) {
        for (int w_offset = 0; w_offset < kW; w_offset++) {
            int i = 0;
            for (long h_col = 0; h_col < inputHeight; ++h_col) {
                for (long w_col = 0; w_col < inputWidth; ++w_col) {
                    int h_im = h_col * strideH - padH + h_offset * dilationH;
                    int w_im = w_col * strideW - padW + w_offset * dilationW;
                    if (h_im >= 0 && h_im < outputHeight &&
                        w_im >= 0 && w_im < outputWidth) {
                        float sum = 0;
                        for (long l = 0; l < k; l++)
                            sum += a[l*lda + i] * b[l*ldb + j];
                        int idx = (c_im * outputHeight + h_im) *
                                  outputWidth + w_im;
                        data_im[idx] += sum;
                    }
                    i++;
                }
            }
            j++;
        }
    }
}
\end{minted}
\captionof{listing}{C Implementation of $transposed_convolution$}
\label{code:transposed_convolution}
\end{code}

\clearpage %force the next chapter to start on a new page. Keep that as the last line of your chapter!
